#Lab 1 ? Bivariate Relations & Simple Regression 

#I. Data
#We will be using a common data set for all lab sessions throughout the quarter. The data set comes from the Census Bureau and uses data collected by the American Community Survey. The unit of analysis is metropolitan cities throughout the United States. There are 517 such observations in the data set. The variables in the data set include: 
#GEO_NAME	Metropolitan and Micropolitan Statistical Area
#areatype	Metropolitan and Micropolitan
#pop_size	Total Estimated Population
#division	US Division
#region	Region
#commute	Mean travel time to work
#hh_chil	Percent households with children <18
#edu_hs	Percentage adults with High School Diploma
#edu_coll	Percentage adults with Bachelor's Degree
#income	Median household income
#income2	Median household income (in thousands)
#no_car	Percentage of households without vehicle
#female	Percent population - female
#age	Median age
#senior	Percent population 65 years or older
#white	Percent Population - White
#black	Percent Population - Black
#indian	Percent Population - Indian
#asian	Percent Population - Asian
#hawaiian	Percent Population - Hawaiian
#other	Percent Population - Other
#hispanic	Percent Population - Hispanic

#The variables for division and region are defined by the map included on the last page. The variable areatype takes on two values ? micro and metro ? depending on the following definitions:
#	A metro area contains a core urban area of 50,000 or more population
#	A micro area contains an urban core of at least 10,000 (but less than 50,000) population
#During the semester we will be answering the research question:
#What best predicts average commute times of US cities?
#There is a mix of qualitative and quantitative predictor variables that we will use to answer the question, using regression based approaches.



# get the working directory 
# set the working directory 
getwd()
setwd("C:\\EDMS651\\Data")

#Install a package and load 
install.packages("psych", dependencies=TRUE)
library(lattice)

#Read data as a.csv file 

census <-read.csv("census.csv", header=TRUE, na.strings="-99", sep=",") 


#Viewing the data and getting summary statistics

str(census)
head(census)
tail(census)
summary(census)


####Multilpe Regression#### 

#Run a multiple linear regression in R

census.lm <- lm(commute~income2 + pop_size, data=census)
summary(census.lm)
names(census.lm)

#Producing ANOVA output of the linear regression model
anova(census.lm)

# Confidence Intervals
# 95% confidence interval for parameters
confint(census.lm)

# 95% confidence interval for predicted values 
predict(census.lm, level = 0.95, interval = "confidence") 
predictedY <- predict(census.lm, level = 0.95, interval = "confidence") 

#verify R^2 = r^2(Y,Yhat) 
cor(census$commute,predictedY[,1])
cor(census$commute,predictedY[,1])^2 
#=.175 


################ assumptions ####

#plot: Observed Y vs predicted Y 
names(census.lm)
census.lm$effects

plot(fitted.values(census.lm)~census$commute, col="black", 
pch=1, bty="l", xlab="Observed Average Commuting Time", 
ylab="Predicted Average Commuting Time", 
xlim=c(0,40),ylim=c(0,40))

plot(fitted.values(census.lm)~census$commute, col="black", 
pch=1, bty="l", xlab="Observed Average Commuting Time", 
ylab="Predicted Average Commuting Time", 
xlim=c(10,40),ylim=c(10,30))


#Plot: Residuals vs X

plot(residuals(census.lm)~census$income2, col="black", 
pch=1, bty="l", xlab="Median income", 
ylab="Residuals", 
xlim=c(0,80),ylim=c(-20,20))

range(census$pop_size)

plot(residuals(census.lm)~census$pop_size, col="black", 
pch=1, bty="l", xlab="Population", 
ylab="Residuals", 
xlim=c(0,20000000),ylim=c(-20,20))

#Plot: Resisulas vs Predicted Y 

plot(residuals(census.lm)~fitted.values(census.lm), col="black", 
pch=1, bty="l", xlab="Predicted Y (commuting time)", 
ylab="Residuals", 
xlim=c(20,25),ylim=c(-20,20))


install.packages("car", dependencies=TRUE)

library(pls)
library(car) 
library(MASS)
# getting stadardized residuals
st.residuals <- stdres(census.lm)
st.residuals 
hist(st.residuals)


###########More formal dignosis 

#Outliers 
outlierTest(census.lm) # Bonferonni p-value for most extreme obs
qqPlot(census.lm, main="QQ Plot") #qq plot for studentized resid
leveragePlots(census.lm) # leverage plots 

# Influential Observations
# added variable plots
avPlots(census.lm)
# Cook's D plot
# identify D values > 4/(n-k-1)
cutoff <- 4/((nrow(census)-length(census.lm$coefficients)-2))
plot(census.lm, which=4, cook.levels=rep(cutoff,length(census$commute)))
# Influence Plot
influencePlot(census.lm, id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )

#multicolinearity 
library(car) 
vif(census.lm)

# The following link will be helpful for more dignosis 
# http://www.statmethods.net/stats/rdiagnostics.html

###Part and Partial Correaitons### 
#check package ppcor# 

install.packages("ppcor", dependencies=TRUE)
library(ppcor)
data <- cbind(census$commute,census$income2,census$pop_size)
#partial#
pcor(data)
#semi partial# 
spcor(data)

### model comparisons ### 

install.packages("lmSupport", dependencies=TRUE)
library(lmSupport)
mod1 <- lm(commute~income2, data=census)
lm.sumSquares(mod1)
mod2 <- lm(commute~income2 + pop_size, data=census)
lm.deltaR2(mod1, mod2)

#### cross-validtion#### 

census.west <-read.csv("census_west.csv", header=TRUE, na.strings="-99", sep=",") 
PsuedoY <- 19.655 + .042*(census.west$income2)+ 9.025*10^(-7)*(census.west$pop_size)
cor(PsuedoY, census.west$commute)
Rshinkage <- 0.175 - (cor(PsuedoY, census.west$commute))^2 
Rshinkage


